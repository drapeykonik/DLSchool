{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMkvLmtFzMIn"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Домашнее задание\n",
    "</b></h3>\n",
    "\n",
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJOOFHKIzPFS"
   },
   "source": [
    "# Часть 1. Vanilla Autoencoder (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNtgixZezVTR"
   },
   "source": [
    "## 1.1 Подготовка данных (0.5 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYZ4wfH-OjsX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tfs\n",
    "import torch\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\", font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EqI7YwzzuFB"
   },
   "outputs": [],
   "source": [
    "def read_attributes(attrs_name = \"lfw_attributes.txt\",\n",
    "                  images_name = \"lfw-deepfunneled\"):\n",
    "    #Download if not exists\n",
    "    if not os.path.exists(images_name):\n",
    "        print(\"images not found, donwloading...\")\n",
    "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
    "        print(\"extracting...\")\n",
    "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
    "        print(\"done\")\n",
    "        assert os.path.exists(images_name)\n",
    "\n",
    "    if not os.path.exists(attrs_name):\n",
    "        print(\"attributes not found, downloading...\")\n",
    "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
    "        print(\"done\")\n",
    "\n",
    "    #Read attrs\n",
    "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "\n",
    "\n",
    "    #Read photos\n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    \n",
    "    #Merge (photos now have same order as attributes)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"Lost some data when merging dataframes\"\n",
    "    #all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBWeB9LT2Rx7",
    "outputId": "c9cad726-5842-4f64-c69a-9e50e4936fa8"
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwBW2JgM2T1L",
    "outputId": "500e61c0-f807-4f68-ff51-6d399e5c3657"
   },
   "outputs": [],
   "source": [
    "%cd DLSCourse/Autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2ehLgUj2Y8u",
    "outputId": "543e3c38-fff9-4d7d-cec0-ff05ca2a55c5"
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAoEycHn1l27"
   },
   "outputs": [],
   "source": [
    "attrs = read_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "99eiihIZe1Sl",
    "outputId": "cabafa56-b689-4818-fa5e-21d4d0014ffb"
   },
   "outputs": [],
   "source": [
    "attrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkcrMcip3Mvo"
   },
   "source": [
    "Разбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vpr-iBLDgww5"
   },
   "source": [
    "Напишем класс датасета, для того, чтобы не загружать все картинки в оперативную память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCf1vx5zg6oU"
   },
   "outputs": [],
   "source": [
    "class FacesDataset(Dataset):\n",
    "  def __init__(self, filenames, size):\n",
    "    self.filenames = filenames\n",
    "    self.size = size\n",
    "    self.transform = tfs.Compose([\n",
    "                                  tfs.CenterCrop(110),\n",
    "                                  tfs.Resize(size=self.size),\n",
    "                                  tfs.ToTensor(),\n",
    "                                  tfs.Normalize(mean=0, std=1)\n",
    "                                  ])\n",
    "  def __len__(self):\n",
    "    return len(self.filenames)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    \n",
    "    image_filename = self.filenames[idx]\n",
    "    image = Image.open(image_filename)\n",
    "    image.load()\n",
    "    image = self.transform(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q4W-qJuirBl"
   },
   "source": [
    "Разделим непосредственно таблицу атрибутов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uP1lOe11ot2"
   },
   "outputs": [],
   "source": [
    "train_attrs, valid_attrs = train_test_split(attrs, train_size=0.9, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bjRUrSC3T8z",
    "outputId": "f5c46d98-953a-48de-8c32-c8d5584ad31e"
   },
   "outputs": [],
   "source": [
    "print(\"Train attributes shape: \", train_attrs.shape)\n",
    "print(\"Valid attributes shape: \", valid_attrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySmjuZl9i3CG"
   },
   "source": [
    "Создадим непосредственно датасеты для обучающей и валидацинной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuwTvKZsi2Zz"
   },
   "outputs": [],
   "source": [
    "train_set = FacesDataset(train_attrs[\"photo_path\"].values, size=128)\n",
    "valid_set = FacesDataset(valid_attrs[\"photo_path\"].values, size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsPaUFKfGn3H"
   },
   "outputs": [],
   "source": [
    "def show_images(ground_truth, reconstructions=None, \n",
    "                first_title=\"Source\", second_title=\"Reconstruction\"):\n",
    "  if reconstructions is None:\n",
    "    size = 1\n",
    "  else: \n",
    "    size = 2\n",
    "  fig = plt.figure(figsize=(5 * ground_truth.shape[0], size * 5))\n",
    "  for i, image in enumerate(ground_truth):\n",
    "    #plt.title(\"Ground truth\")\n",
    "    plt.subplot(size, ground_truth.shape[0], i + 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(first_title)\n",
    "    plt.imshow(image.permute((1, 2, 0)).numpy())\n",
    "\n",
    "  if reconstructions is not None:\n",
    "    for i, image in enumerate(reconstructions):\n",
    "    #plt.title(\"Ground truth\")\n",
    "      plt.subplot(size, ground_truth.shape[0], ground_truth.shape[0] + i + 1)\n",
    "      plt.grid(False)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.title(second_title)\n",
    "      plt.imshow(image.permute((1, 2, 0)).cpu().detach().numpy())\n",
    "  plt.ioff()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0XRGfmWmR8j"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([train_set[i] for i in np.random.randint(0, len(train_set), size=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "n-lmNz9dJosq",
    "outputId": "14f3ea6b-be68-4678-8d05-05dfd3c6be96"
   },
   "outputs": [],
   "source": [
    "show_images(examples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ8yjUt2V8n3"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Faa1D4hV4fIC"
   },
   "source": [
    "## 1.2 Архитектура модели (1.5 балла)\n",
    "В этом разделе напишем и обучим автоэнкодер\n",
    "<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5kdN3YM3-_C"
   },
   "outputs": [],
   "source": [
    "#Latent space's dimension\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PegH8lSnSW_"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, padding=0):\n",
    "    super().__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.padding = padding\n",
    "    self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                          kernel_size=3, padding=self.padding)\n",
    "    self.bn = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.bn(x)\n",
    "    x = F.elu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Nwstqso46xM"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        #nn.Conv2d(in_channels=8, out_channels=16, kernel_size=2, stride=2),\n",
    "\n",
    "        ConvBlock(in_channels=3, out_channels=32), # 128 -> 126\n",
    "        ConvBlock(in_channels=32, out_channels=32), # 126 -> 124\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 124 -> 62\n",
    "        ConvBlock(in_channels=32, out_channels=64), # 62 -> 60\n",
    "        ConvBlock(in_channels=64, out_channels=64), # 60 -> 58\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 58 -> 29\n",
    "        ConvBlock(in_channels=64, out_channels=128), # 29 -> 27\n",
    "        ConvBlock(in_channels=128, out_channels=128), # 27 -> 25\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 25 -> 12\n",
    "        ConvBlock(in_channels=128, out_channels=256), # 12 -> 10\n",
    "        ConvBlock(in_channels=256, out_channels=256), # 10 -> 8\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 8 -> 4\n",
    "        ConvBlock(in_channels=256, out_channels=256), # 4 -> 2\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=2, stride=2),\n",
    "        nn.ELU(),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Unflatten(dim=1, unflattened_size=(512, 1, 1)),\n",
    "        nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 1 -> 2\n",
    "        ConvBlock(in_channels=256, out_channels=256, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 2 -> 4\n",
    "        ConvBlock(in_channels=256, out_channels=256, padding=1),\n",
    "        ConvBlock(in_channels=256, out_channels=128, padding=1), \n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        ConvBlock(in_channels=128, out_channels=128, padding=1),\n",
    "        ConvBlock(in_channels=128, out_channels=64, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 8 -> 16\n",
    "        ConvBlock(in_channels=64, out_channels=64, padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 16 -> 32\n",
    "        ConvBlock(in_channels=32, out_channels=32, padding=1),\n",
    "        ConvBlock(in_channels=32, out_channels=16, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 32 -> 64\n",
    "        ConvBlock(in_channels=16, out_channels=16, padding=1),\n",
    "        ConvBlock(in_channels=16, out_channels=3, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 64 -> 128\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    latent_code = self.encoder(x)\n",
    "    reconstruction = self.decoder(latent_code)\n",
    "    return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umhv5B3SRdPu"
   },
   "outputs": [],
   "source": [
    "class AutoencoderV2(torch.nn.Module):\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        ConvBlock(in_channels=3, out_channels=16),\n",
    "        ConvBlock(in_channels=16, out_channels=16),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=16, out_channels=32),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=32, out_channels=64),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=64, out_channels=64),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=64, out_channels=128),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=4*4*128, out_features=self.latent_dim),\n",
    "        nn.ELU()\n",
    "\n",
    "    )\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=4*4*128),\n",
    "        nn.ELU(),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(128, 4, 4)),\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        ConvBlock(in_channels=128, out_channels=64, padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=64, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, # 8 -> 16\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, # 16 -> 32\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=32, out_channels=16, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, # 32 -> 64\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=16, out_channels=8, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=3, # 64 -> 128\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        nn.Sigmoid()\n",
    "\n",
    "    )\n",
    "\n",
    "  def forward(self, sample):\n",
    "    latent = self.encoder(sample)\n",
    "    reconstructed = self.decoder(latent)\n",
    "    return reconstructed, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFALTEdOEm_f",
    "outputId": "1464719d-ed53-42be-ba2b-34be0c86e4ec"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Training device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP8lDbC3BKsD"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "autoencoder_mse = Autoencoder().to(device)\n",
    "optimizer = optim.Adam(autoencoder_mse.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsjKqhngBTD0",
    "outputId": "c908861e-3178-40c8-98ef-8a5f5bfde249"
   },
   "outputs": [],
   "source": [
    "summary(model=autoencoder_mse, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4v-PHo1GcX5"
   },
   "source": [
    "## Обучение (2 балла)\n",
    "Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n",
    "\n",
    "А, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3AujdkLCuHP"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, train_loader, summary_writer=None):\n",
    "  train_losses_epoch = []\n",
    "\n",
    "  model.train()\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    reconstruction, latent_code = model(batch.to(device).float())\n",
    "    loss = criterion(reconstruction, batch.to(device).float())\n",
    "    train_losses_epoch.append(loss.item())\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalar(\"Epoch. Train loss\", loss.item(), i)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  return train_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uf_KlRzmLX5H"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, criterion, optimizer, valid_loader, summary_writer=None):\n",
    "  valid_losses_epoch = []\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "      reconstruction, latent_code = model(batch.to(device).float())\n",
    "      loss = criterion(reconstruction, batch.to(device).float())\n",
    "      valid_losses_epoch.append(loss.item())\n",
    "      if summary_writer is not None:\n",
    "        summary_writer.add_scalar(\"Epoch. Valid loss\", loss.item(), i)\n",
    "\n",
    "  return valid_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySt6yJlfwfgH"
   },
   "outputs": [],
   "source": [
    "def visualize(examples, reconstructions, train_losses, valid_losses):\n",
    "  #plt.ion()\n",
    "  figure = plt.figure(constrained_layout=True, figsize=(32, 8))\n",
    "  subfigs = figure.subfigures(1, 2, wspace=0.07)\n",
    "  axs_left = subfigs[0].subplots(2, 5) \n",
    "  ax = subfigs[1].subplots(1, 1)\n",
    "  for j in range(5):\n",
    "      axs_left[0, j].clear()\n",
    "      axs_left[0, j].imshow(examples[j].permute((1, 2, 0)).numpy())\n",
    "      axs_left[1, j].clear()\n",
    "      axs_left[1, j].imshow(reconstructions[j].permute((1, 2, 0)).cpu().numpy())\n",
    "      for i in range(2):\n",
    "        axs_left[i, j].set_xticks([])\n",
    "        axs_left[i, j].set_yticks([])\n",
    "      axs_left[0, j].set_title(\"Source\")\n",
    "      axs_left[1, j].set_title(\"Reconstruction\")\n",
    "\n",
    "  ax.clear()\n",
    "  ax.plot(train_losses, label=\"Train\")\n",
    "  ax.plot(valid_losses, label=\"Validation\")\n",
    "  ax.set_title(\"Training AE\", fontsize=18) \n",
    "  ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "  ax.set_ylabel(\"Loss value\", fontsize=14)\n",
    "  ax.legend()\n",
    "  figure.canvas.draw()\n",
    "  figure.canvas.flush_events()\n",
    "  plt.show()\n",
    "  return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQ1tZDspNU8U"
   },
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, train_loader, valid_loader, epochs, summary_writer=None):\n",
    "  train_losses, valid_losses = [], []\n",
    "\n",
    "  pbar = tqdm(range(epochs))\n",
    "  pbar.set_description(\"Epoch 1\")\n",
    "  for epoch in pbar:\n",
    "    if epoch != 0:\n",
    "      pbar.set_description(f\"Epoch {epoch + 1}. \\\n",
    "      Train loss: {round(train_losses[-1], 4)}. \\\n",
    "      Valid loss: {round(valid_losses[-1], 4)}\")\n",
    "\n",
    "    train_losses_epoch = train_epoch(model, criterion, optimizer, \n",
    "                                     train_loader, summary_writer)\n",
    "    valid_losses_epoch = valid_epoch(model, criterion, optimizer, \n",
    "                                     valid_loader, summary_writer)\n",
    "    \n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "    valid_losses.append(np.mean(valid_losses_epoch))\n",
    "\n",
    "    examples = torch.stack([valid_set[i] for i in np.random.randint(0, len(valid_set), size=5)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      reconstructions, latent_codes = model(examples.to(device).float())\n",
    "\n",
    "    figure = visualize(examples, reconstructions, train_losses, valid_losses)\n",
    "\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalars(\"Training\", {\"Train\" : train_losses[-1],\n",
    "                                             \"Valid\" : valid_losses[-1]}, epoch)\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        summary_writer.add_figure(f\"Reconstruction. Epoch {epoch + 1}\", figure)\n",
    "    \n",
    "  return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPXLul_SU7dj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "writer = tensorboard.SummaryWriter(\"Vanilla AE v.1. Experiments/MSELoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461,
     "referenced_widgets": [
      "0cd6bcaa7a754b70bf05e0598db14eba",
      "133c1353bdc04d8fb414a9e5ce555e0c",
      "7fc0452c80d747d6a6973abc9c52b3de",
      "d3e58903291b4ed98e3764c462db452b",
      "e3e63a5324214cd8bbce423c4caf7e5d",
      "538fe3f2af4243e69c4b503f63eb932c",
      "10821e7d9ac34f59b40b724e7d76866f",
      "2b38392c574c4d31bce53dfcd5230c02",
      "510e15337cc04519a1079db58f24f32d",
      "33ae86bfe41a4ad89ad8592b2114c9b5",
      "d38c3bb958cb4b31b39f775cf00a9bc0"
     ]
    },
    "id": "6gLw6cZPVuSK",
    "outputId": "28a6bdd3-b45a-4719-f4e9-c75c649ed3e8"
   },
   "outputs": [],
   "source": [
    "train_losses_mse, valid_losses_mse = fit(autoencoder_mse, criterion, optimizer, train_loader, valid_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "_kA1lqXeQGET",
    "outputId": "d8486f3f-e9de-41a0-cd56-d50472a03d04"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(train_losses_mse, label=\"Train\")\n",
    "plt.plot(valid_losses_mse, label=\"Valid\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.title(\"Training Vanilla AE (MSE)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PW_aEKuIQGCo"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "autoencoder_v2_mse = AutoencoderV2().to(device)\n",
    "optimizer = optim.Adam(autoencoder_v2_mse.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxFwJ-_wU663",
    "outputId": "5d75f66a-df11-447e-faed-abdcd0d582ae"
   },
   "outputs": [],
   "source": [
    "summary(model=autoencoder_v2_mse, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eCpXIrDUJI_"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "writer = tensorboard.SummaryWriter(\"Vanilla AE v.2. Experiments/MSELoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b9241467505f4e62988e34394bcf1ee3",
      "04d1ee6b1bf44a3dac6cf7ab93e65275",
      "b7b0b581ceac4572a8f35dbeac9d8533",
      "a508d397f20f4e399eb168c139455e74",
      "589bcf5b03fe4e1fb8ecc385839c75ad",
      "5d7bc3751a94498596631e05072b8d8e",
      "0ef8572a2b6f4bc4bad2cd2c9ae3be0e",
      "dc80c74e3e05415a9db7bdee33af78f0",
      "27ea256f6a92472d95901c21cb4001bb",
      "834402d19f234cf4bbfe56287199cce8",
      "56e45dc638e34e639c0c1601be26eb3d"
     ]
    },
    "id": "iEnvZGwbUJI_",
    "outputId": "acb6cd6e-9e38-4760-dde4-a5dce81f4deb"
   },
   "outputs": [],
   "source": [
    "train_losses_v2_mse, valid_losses_v2_mse = fit(autoencoder_v2_mse, criterion, optimizer, train_loader, valid_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "G6ZyJpzwUAQy",
    "outputId": "c2d00242-626d-4e3e-97b3-deb01c264646"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(train_losses_v2_mse, label=\"Train\")\n",
    "plt.plot(valid_losses_v2_mse, label=\"Valid\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.title(\"Training Vanilla AE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1rJOpSY6wjI"
   },
   "source": [
    "Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrM97A8HgyxV"
   },
   "source": [
    "### Autoencoder v.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "TZ-Xb1WQJgR3",
    "outputId": "7d299298-ac60-4888-98e8-0442740de6e1"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([valid_set[i] for i in range(5)])\n",
    "reconstructions, latent_codes = autoencoder_mse(examples.to(device).float())\n",
    "show_images(examples, reconstructions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0vhf8HFg5zc"
   },
   "source": [
    "### Autoencoder v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "f2871eEMg9Ad",
    "outputId": "19d0905d-f34f-4e50-8ec4-9283b51a4962"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([valid_set[i] for i in range(5)])\n",
    "reconstructions, latent_codes = autoencoder_v2_mse(examples.to(device).float())\n",
    "show_images(examples, reconstructions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnlSvK1l8tl8"
   },
   "source": [
    "## 1.4. Sampling (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s74QeI6b8zjz"
   },
   "source": [
    "Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n",
    "\n",
    "Давайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n",
    "\n",
    "__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU8sXKj6hxdK"
   },
   "source": [
    "### Autoencoder v.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiRCo-_diAgi"
   },
   "source": [
    "Попробуем сэмплировать из стандартного нормального распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iwWNxM7SiAgo",
    "outputId": "c807832b-a6da-4318-b915-bd51104c5480"
   },
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "z = torch.tensor(np.random.randn(num_samples, LATENT_DIM)).float().to(device)\n",
    "output = autoencoder_mse.decoder(z).cpu().detach()\n",
    "for i in range(num_samples // 5):\n",
    "  show_images(output[i*5:(i+1)*5, :, :, :], first_title=\"Generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKLcrSLYiAgo"
   },
   "source": [
    "Теперь попробуем привести распределение к нормальному с параметрами, вычисленными по латентным векторам обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGBVk7cCiAgo"
   },
   "outputs": [],
   "source": [
    "autoencoder_mse.eval()\n",
    "with torch.no_grad():\n",
    "  latent_vectors = []\n",
    "  for image in train_loader.dataset:\n",
    "    reconstructions, latent = autoencoder_mse(image[None, :, :, :].to(device).float())\n",
    "    latent_vectors.append(latent.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeDDFytbiAgo",
    "outputId": "43b41d5c-6e78-442b-f5d6-2f0a32e832f7"
   },
   "outputs": [],
   "source": [
    "len(latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkm_QCL7iAgo",
    "outputId": "983a6ea8-9c38-4e0a-d54e-3f440102d47f"
   },
   "outputs": [],
   "source": [
    "latent_vectors = np.stack(latent_vectors)\n",
    "latent_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0kDA31ziAgo"
   },
   "outputs": [],
   "source": [
    "latent_mean = latent_vectors.mean(axis=0)\n",
    "latent_std = latent_vectors.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf4oLA0FiAgp",
    "outputId": "1f84f645-6829-4a55-988e-ed081d575deb"
   },
   "outputs": [],
   "source": [
    "latent_mean.shape, latent_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q36RYRjxiAgp",
    "outputId": "450ca5ae-46bb-46af-d79f-61c631f3e46d"
   },
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "z = torch.tensor(np.random.normal(latent_mean, latent_std, (num_samples, LATENT_DIM))).float().to(device)\n",
    "output = autoencoder_mse.decoder(z).cpu().detach()\n",
    "for i in range(num_samples // 5):\n",
    "  show_images(output[i*5:(i+1)*5, :, :, :], first_title=\"Generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRMnP7gJhtTh"
   },
   "source": [
    "### Autoencoder v.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBzfAKl9NdR5"
   },
   "source": [
    "Попробуем сэмплировать из стандартного нормального распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TIDO00wI8xLJ",
    "outputId": "d1650758-57e7-42d8-c5e1-835e27719f0d"
   },
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "z = torch.tensor(np.random.randn(num_samples, LATENT_DIM)).float().to(device)\n",
    "output = autoencoder_v2_mse.decoder(z).cpu().detach()\n",
    "for i in range(num_samples // 5):\n",
    "  show_images(output[i*5:(i+1)*5, :, :, :], first_title=\"Generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khn9H2Q7NlqP"
   },
   "source": [
    "Теперь попробуем привести распределение к нормальному с параметрами, вычисленными по латентным векторам обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwNNHoteEm8x"
   },
   "outputs": [],
   "source": [
    "autoencoder_v2_mse.eval()\n",
    "with torch.no_grad():\n",
    "  latent_vectors = []\n",
    "  for image in train_loader.dataset:\n",
    "    reconstructions, latent = autoencoder_v2_mse(image[None, :, :, :].to(device).float())\n",
    "    latent_vectors.append(latent.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_7vGCqeHyco",
    "outputId": "fcdbb428-58fe-46db-f99e-d6c9a88fe8a7"
   },
   "outputs": [],
   "source": [
    "len(latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iTcVlPgGrQO",
    "outputId": "d6cb231e-6a7e-4286-cd95-9703108062e7"
   },
   "outputs": [],
   "source": [
    "latent_vectors = np.stack(latent_vectors)\n",
    "latent_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7B2cplYQIy1m"
   },
   "outputs": [],
   "source": [
    "latent_mean = latent_vectors.mean(axis=0)\n",
    "latent_std = latent_vectors.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgXHNqlRJHfc",
    "outputId": "db1ffe94-bfa7-4d55-ed1e-ea8917dfebd7"
   },
   "outputs": [],
   "source": [
    "latent_mean.shape, latent_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sD8H-6SEJLR8",
    "outputId": "e55f24ee-16d9-4d7e-efa4-3795269e6194"
   },
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "z = torch.tensor(np.random.normal(latent_mean, latent_std, (num_samples, LATENT_DIM))).float().to(device)\n",
    "output = autoencoder_mse.decoder(z).cpu().detach()\n",
    "for i in range(num_samples // 5):\n",
    "  show_images(output[i*5:(i+1)*5, :, :, :], first_title=\"Generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qUXbgkklKC"
   },
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr3-LFogjC3A"
   },
   "source": [
    "Хотелось бы отметить, что вторая версия лучше восстанавливает картинки, но первая лучше справляется с генерацией картинок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlHiURgL-fQ0"
   },
   "source": [
    "## Time to make fun! (4 балла)\n",
    "Давайте научимся пририсовывать людям улыбки =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YKIcKy4-xOh"
   },
   "source": [
    "<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMP8QQch-0PK"
   },
   "source": [
    "План такой:\n",
    "\n",
    "1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n",
    "\n",
    "Найти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n",
    "\n",
    "2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n",
    "\n",
    "3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n",
    "\n",
    "4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "FdzedDlgMOGs",
    "outputId": "f671ae94-0764-4f10-ae7a-c0b2b272935f"
   },
   "outputs": [],
   "source": [
    "attrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B820Li3xM5fX"
   },
   "source": [
    "Конечно же желательно рассматривать людей одного пола при формировании улыбки. Рассмотрим улыбающихся и грустных мужчин  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsaiUoWvjxcH"
   },
   "source": [
    "### Autoencoder v.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDAO9C9Ej4m-"
   },
   "outputs": [],
   "source": [
    "smiling_men_indices = train_attrs[train_attrs[\"Male\"] > 1].sort_values(by=\"Smiling\", ascending=False).index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubUXb5Eij4m_"
   },
   "outputs": [],
   "source": [
    "sad_men_indices = train_attrs[train_attrs[\"Male\"] > 1].sort_values(by=\"Smiling\", ascending=True).index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "-8SAwCrDj4m_",
    "outputId": "d334d819-780d-47af-d562-01a8aa60a781"
   },
   "outputs": [],
   "source": [
    "smiling_examples = torch.stack([train_set[i] for i in smiling_men_indices])\n",
    "sad_examples = torch.stack([train_set[i] for i in sad_men_indices])\n",
    "show_images(smiling_examples[:10], sad_examples[:10], first_title=\"Smile\", second_title=\"Sad\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLNcXedhj4m_"
   },
   "outputs": [],
   "source": [
    "_, smiling_men_vectors = autoencoder_mse(smiling_examples.to(device).float())\n",
    "_, sad_men_vectors = autoencoder_mse(sad_examples.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whl4N3qhj4m_"
   },
   "outputs": [],
   "source": [
    "smile_vector = smiling_men_vectors.mean(axis=0) - sad_men_vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHG1Bf-Gj4m_",
    "outputId": "b30bdc1a-a5b4-45d0-b6cc-82570a50dc1d"
   },
   "outputs": [],
   "source": [
    "smile_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbmk1HjPj4m_"
   },
   "outputs": [],
   "source": [
    "changed_men = autoencoder_mse.decoder(sad_men_vectors + smile_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fh8wdSDQj4m_",
    "outputId": "4f57c073-a3df-4a8b-f7d1-4282bf5e8476"
   },
   "outputs": [],
   "source": [
    "changed_men.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "rxkRPL4Hj4m_",
    "outputId": "a3350c44-d48a-4629-9737-1bc4ec8d1eac"
   },
   "outputs": [],
   "source": [
    "show_images(sad_examples[:10], changed_men[:10], first_title=\"Source\", second_title=\"Changed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRGgvqozjtrw"
   },
   "source": [
    "###| Autorncoder v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "s4ZcrGEdELqX",
    "outputId": "43b6ac58-d80a-49f6-c49f-85675a78a4ad"
   },
   "outputs": [],
   "source": [
    "smiling_examples = torch.stack([train_set[i] for i in smiling_men_indices])\n",
    "sad_examples = torch.stack([train_set[i] for i in sad_men_indices])\n",
    "show_images(smiling_examples[:10], sad_examples[:10], first_title=\"Smile\", second_title=\"Sad\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueY860DjFQYr"
   },
   "outputs": [],
   "source": [
    "_, smiling_men_vectors = autoencoder_v2_mse(smiling_examples.to(device).float())\n",
    "_, sad_men_vectors = autoencoder_v2_mse(sad_examples.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XenpJh-pFo_7"
   },
   "outputs": [],
   "source": [
    "smile_vector = smiling_men_vectors.mean(axis=0) - sad_men_vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fA51nHoGJUF",
    "outputId": "7844262a-a89e-4b4a-c887-bffeca51f682"
   },
   "outputs": [],
   "source": [
    "smile_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahjVc3_vGMgF"
   },
   "outputs": [],
   "source": [
    "changed_men = autoencoder_v2_mse.decoder(sad_men_vectors + smile_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRGjG5PeGdXe",
    "outputId": "dce86555-be73-4bd0-e383-093d4e008f37"
   },
   "outputs": [],
   "source": [
    "changed_men.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "ZaSsYvudGfok",
    "outputId": "deba9e1d-1711-4f7d-a1be-4799cddacb70"
   },
   "outputs": [],
   "source": [
    "show_images(sad_examples[:10], changed_men[:10], first_title=\"Source\", second_title=\"Changed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4NPOtYy-37h"
   },
   "source": [
    "Вуаля! Вы восхитительны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iRguBnq-17Q"
   },
   "source": [
    "Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-BJ2r2wkTWn"
   },
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK2xk7yikXXc"
   },
   "source": [
    "Первая версия лучше справилась с добавлением улыбки (лица все таки более правдоподобные и без помех)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKF_GDuk10J"
   },
   "source": [
    "# Часть 2: Variational Autoencoder (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QjJIlT9k-cv"
   },
   "source": [
    "Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcfZkn-3k83k"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "train_set = datasets.MNIST(root=\"./MNIST/\", train=True, transform=tfs.ToTensor(), download=True)\n",
    "test_set = datasets.MNIST(root=\"./MNIST/\", train=False, transform=tfs.ToTensor(), download=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3BW_E_5OMrC"
   },
   "source": [
    "## 2.1 Архитектура модели и обучение (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPeSIc3pP2eC"
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQkwmO1iOSdL"
   },
   "source": [
    "Реализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVl8rpk_V-D8"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, padding=0):\n",
    "    super().__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.padding = padding\n",
    "    self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                          kernel_size=3, padding=self.padding)\n",
    "    self.bn = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.bn(x)\n",
    "    x = F.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZUScEbONNrG"
   },
   "outputs": [],
   "source": [
    "class VAE_CNN(nn.Module):\n",
    "\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        ConvBlock(in_channels=1, out_channels=32), #28 -> 26\n",
    "        ConvBlock(in_channels=32, out_channels=32), #26 -> 24\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(), # 24 -> 12\n",
    "        ConvBlock(in_channels=32, out_channels=32), # 12 -> 10\n",
    "        ConvBlock(in_channels=32, out_channels=32), # 10 -> 8\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2), # 8 -> 4\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=4*4*32, out_features=2*self.latent_dim),\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=32*4*4),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(32, 4, 4)),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 8 -> 16\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=32), # 16 -> 14\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, # 14 -> 28\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=1, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def reparameterize(self, mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    sample = std * eps + mu\n",
    "    return sample\n",
    "\n",
    "  def get_latent_vector(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    return z\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction, mu, log_var\n",
    "\n",
    "  def sample(self, z):\n",
    "    generated = self.decoder(z)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilIW2QnXus2x"
   },
   "outputs": [],
   "source": [
    "class VAE_CNN(nn.Module):\n",
    "\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=4), # 28 -> 25\n",
    "        nn.BatchNorm2d(num_features=8),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4), # 25 -> 22\n",
    "        nn.BatchNorm2d(num_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4), # 22 -> 19\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4), # 19 -> 16\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2), # 16 -> 8\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2), # 8 -> 4\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=32*4*4, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=2*self.latent_dim),\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=128, out_features=32*4*4),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(32, 4, 4)),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 8 -> 16\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=32), # 16 -> 14\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, # 14 -> 28\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=1, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def reparameterize(self, mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    sample = std * eps + mu\n",
    "    return sample\n",
    "\n",
    "  def get_latent_vector(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    return z\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction, mu, log_var\n",
    "\n",
    "  def sample(self, z):\n",
    "    generated = self.decoder(z)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ffr-RAtyt-no"
   },
   "outputs": [],
   "source": [
    "class VAE_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=28*28, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=128, out_features=2*self.latent_dim)\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=28*28),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n",
    "    )\n",
    "\n",
    "  def reparameterize(self, mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    sample = std * eps + mu\n",
    "    return sample\n",
    "\n",
    "  def get_latent_vector(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    return z\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction, mu, log_var\n",
    "\n",
    "  def sample(self, z):\n",
    "    generated = self.decoder(z)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbX9GCnzrk_W"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFge2cBKdGFS"
   },
   "outputs": [],
   "source": [
    "vae_cnn = VAE_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ol75TpeQdW0k",
    "outputId": "b8ff7cdf-f0c3-472a-c4e1-a1edeeefcfe2"
   },
   "outputs": [],
   "source": [
    "summary(model=vae, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xuyKBB88bd-"
   },
   "outputs": [],
   "source": [
    "vae_fc = VAE_FC().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5d0NWCS8gMV",
    "outputId": "c75f2b48-378e-4326-ff04-bb1e027a1b36"
   },
   "outputs": [],
   "source": [
    "summary(model=vae_fc, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQpbIAkqOhCW"
   },
   "source": [
    "Определим лосс и его компоненты для VAE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBsO7XIxPOpf"
   },
   "source": [
    "Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n",
    "\n",
    "Общий лосс будет выглядеть так:\n",
    "\n",
    "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
    "\n",
    "Формула для KL-дивергенции:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
    "\n",
    "В качестве log-likelihood возьмем привычную нам кросс-энтропию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCr0BxIqOh1Q"
   },
   "outputs": [],
   "source": [
    "def KL_divergence(mu, log_var):\n",
    "  loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "  return loss\n",
    "\n",
    "def log_likelihood(x, reconstruction):\n",
    "  loss = nn.BCELoss(reduction=\"sum\")\n",
    "  return loss(reconstruction, x)\n",
    "\n",
    "def loss_vae(x, reconstruction, mu, log_var):\n",
    "  return KL_divergence(mu, log_var) + log_likelihood(x, reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4et8xyCRgUGv"
   },
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOvoolExhgDj"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, train_loader, summary_writer=None):\n",
    "  train_losses_epoch = []\n",
    "\n",
    "  model.train()\n",
    "  for i, (batch, _) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    reconstruction, mu, log_var = model(batch.to(device).float())\n",
    "    loss = criterion(batch.to(device).float(), reconstruction, mu, log_var)\n",
    "    train_losses_epoch.append(loss.item())\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalar(\"Epoch. Train loss\", loss.item(), i)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  return train_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHo_MI7zhgD9"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, criterion, optimizer, valid_loader, summary_writer=None):\n",
    "  valid_losses_epoch = []\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, (batch, _) in enumerate(valid_loader):\n",
    "      reconstruction, mu, log_var = model(batch.to(device).float())\n",
    "      loss = criterion(batch.to(device).float(), reconstruction, mu, log_var)\n",
    "      valid_losses_epoch.append(loss.item())\n",
    "      if summary_writer is not None:\n",
    "        summary_writer.add_scalar(\"Epoch. Valid loss\", loss.item(), i)\n",
    "\n",
    "  return valid_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNzcvlVZhgD-"
   },
   "outputs": [],
   "source": [
    "def visualize(examples, reconstructions, train_losses, valid_losses):\n",
    "  #plt.ion()\n",
    "  figure = plt.figure(constrained_layout=True, figsize=(32, 8))\n",
    "  subfigs = figure.subfigures(1, 2, wspace=0.07)\n",
    "  axs_left = subfigs[0].subplots(2, 5) \n",
    "  ax = subfigs[1].subplots(1, 1)\n",
    "  for j in range(5):\n",
    "      axs_left[0, j].clear()\n",
    "      axs_left[0, j].imshow(examples[j].view(28, 28).numpy())\n",
    "      axs_left[1, j].clear()\n",
    "      axs_left[1, j].imshow(reconstructions[j].view(28, 28).cpu().numpy())\n",
    "      for i in range(2):\n",
    "        axs_left[i, j].set_xticks([])\n",
    "        axs_left[i, j].set_yticks([])\n",
    "      axs_left[0, j].set_title(\"Source\")\n",
    "      axs_left[1, j].set_title(\"Reconstruction\")\n",
    "\n",
    "  ax.clear()\n",
    "  ax.plot(train_losses, label=\"Train\")\n",
    "  ax.plot(valid_losses, label=\"Validation\")\n",
    "  ax.set_title(\"Training AE\", fontsize=18) \n",
    "  ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "  ax.set_ylabel(\"Loss value\", fontsize=14)\n",
    "  ax.legend()\n",
    "  #figure.canvas.draw()\n",
    "  #figure.canvas.flush_events()\n",
    "  plt.show()\n",
    "  return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hOjc1u8hgD_"
   },
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, train_loader, valid_loader, epochs, summary_writer=None):\n",
    "  train_losses, valid_losses = [], []\n",
    "\n",
    "  pbar = tqdm(range(epochs))\n",
    "  pbar.set_description(\"Epoch 1\")\n",
    "  for epoch in pbar:\n",
    "    if epoch != 0:\n",
    "      pbar.set_description(f\"Epoch {epoch + 1}. \\\n",
    "      Train loss: {round(train_losses[-1], 4)}. \\\n",
    "      Valid loss: {round(valid_losses[-1], 4)}\")\n",
    "\n",
    "    train_losses_epoch = train_epoch(model, criterion, optimizer, \n",
    "                                     train_loader, summary_writer)\n",
    "    valid_losses_epoch = valid_epoch(model, criterion, optimizer, \n",
    "                                     valid_loader, summary_writer)\n",
    "    \n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "    valid_losses.append(np.mean(valid_losses_epoch))\n",
    "\n",
    "    examples = torch.stack([valid_loader.dataset[i][0] \n",
    "                            for i in np.random.randint(0, len(valid_loader.dataset), size=5)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      reconstructions, _, _ = model(examples.to(device).float())\n",
    "\n",
    "    figure = visualize(examples, reconstructions, train_losses, valid_losses)\n",
    "\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalars(\"Training\", {\"Train\" : train_losses[-1],\n",
    "                                             \"Valid\" : valid_losses[-1]}, epoch)\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        summary_writer.add_figure(f\"Reconstruction. Epoch {epoch + 1}\", figure)\n",
    "    \n",
    "  return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hVCThm78pv9"
   },
   "source": [
    "### CNN VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bw9Kz47HgO4f"
   },
   "outputs": [],
   "source": [
    "vae_cnn = VAE_CNN().to(device)\n",
    "criterion = loss_vae\n",
    "optimizer = optim.Adam(params=vae_cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7M6iAjxzgj-v"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "writer = tensorboard.SummaryWriter(log_dir=\"VAE Experiments/CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "975851b980b24a1089d868910cac516a",
      "a9deff164f6f436090726ac266284eb0",
      "ee6bc4f982ef47bf8858fee38cf8119d",
      "73c26c0a0b344636b69739aff5dbe9c3",
      "06ba3ff55e3e4979abb92b91602b8db8",
      "3473495fc1f046719fdec3a8939b8efa",
      "c3ce46f6cc0f4167891879f8c927a431",
      "89e4b6e357014d75bdaa6c62eec16720",
      "7b037ed839064c779d35fa213d74cb2a",
      "ea1b939539d44948886fa4c2c2028d85",
      "910f162b015547278e3da9f70b13e1e2"
     ]
    },
    "id": "myZ14tgVg3J_",
    "outputId": "1d0e78d3-3369-42ab-d84f-e481b842dd68"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses = fit(vae_cnn, criterion, optimizer, \n",
    "                                 train_loader, test_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXQW5pSf6HEJ"
   },
   "source": [
    "Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dV_V3FVm6ijJ"
   },
   "outputs": [],
   "source": [
    "def show_images(ground_truth, reconstructions=None, \n",
    "                first_title=\"Source\", second_title=\"Reconstruction\"):\n",
    "  if reconstructions is None:\n",
    "    size = 1\n",
    "  else: \n",
    "    size = 2\n",
    "  fig = plt.figure(figsize=(5 * ground_truth.shape[0], size * 5))\n",
    "  for i, image in enumerate(ground_truth):\n",
    "    #plt.title(\"Ground truth\")\n",
    "    plt.subplot(size, ground_truth.shape[0], i + 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(first_title)\n",
    "    plt.imshow(image.view(28, 28).numpy())\n",
    "\n",
    "  if reconstructions is not None:\n",
    "    for i, image in enumerate(reconstructions):\n",
    "    #plt.title(\"Ground truth\")\n",
    "      plt.subplot(size, ground_truth.shape[0], ground_truth.shape[0] + i + 1)\n",
    "      plt.grid(False)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.title(second_title)\n",
    "      plt.imshow(image.view(28, 28).cpu().detach().numpy())\n",
    "  plt.ioff()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "PEisTmJ5hO0S",
    "outputId": "1b2aff4f-bf01-429c-e0d0-00239f8bf4ae"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([test_set[i][0] for i in range(5)])\n",
    "reconstructions, _, _ = vae_cnn(examples.to(device).float())\n",
    "show_images(examples, reconstructions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQXYIXjoYY8F"
   },
   "source": [
    "Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "bOhhH-osYY8G",
    "outputId": "f2f2bff6-1006-459c-a0c4-7e18c6868ca1"
   },
   "outputs": [],
   "source": [
    "z = torch.tensor(np.array([np.random.normal(0, 1, LATENT_DIM) for i in range(10)])).float().to(device)\n",
    "output = vae_cnn.sample(z).cpu().detach()\n",
    "show_images(output, first_title=\"Generated\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP8_lnWF8-qX"
   },
   "source": [
    "### FC VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOD3dXzx9AlN"
   },
   "outputs": [],
   "source": [
    "vae_fc = VAE_FC().to(device)\n",
    "criterion = loss_vae\n",
    "optimizer = optim.Adam(params=vae_fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGutsW5v9AlO"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "writer = tensorboard.SummaryWriter(log_dir=\"VAE Experiments/FC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a5d1ff60f8294372aa8d5371d0cc32de",
      "4767fdd51e70431fb028e526d1e11931",
      "c3394a2861e3495a8906f7ea7c82ccd3",
      "c0033c4c7410499d8ee0d0fcfdd1a2ec",
      "97c20e9edc854213a755b1529ad9866d",
      "a8d02a2a31cc45a988b64e34b4596f4c",
      "628ae6705d4b4bf9b3d711b6b00a27b8",
      "1b03a157d15041578f6c7ee059d51470",
      "7ff34cdba50d48b1aa9c9ae37e2e1877",
      "f5f74fdcfc7b43ac8afb0f520bf9502e",
      "4aab3f8f97114dc299aabd7b2fab5013"
     ]
    },
    "id": "Yl8GLgBb9AlO",
    "outputId": "63e54fdf-2259-4a17-cfa5-614fd580a81f"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses = fit(vae_fc, criterion, optimizer, \n",
    "                                 train_loader, test_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "YCZI1iZF9_jx",
    "outputId": "b2c8bbd4-81c4-41da-fbba-7a65055d9703"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([test_set[i][0] for i in range(5)])\n",
    "reconstructions, _, _ = vae_fc(examples.to(device).float())\n",
    "show_images(examples, reconstructions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "lzoe78cs9_jz",
    "outputId": "e38a12a2-f4ef-4184-e64a-08e569ba15e0"
   },
   "outputs": [],
   "source": [
    "z = torch.tensor(np.array([np.random.normal(0, 1, LATENT_DIM) for i in range(10)])).float().to(device)\n",
    "output = vae_fc.sample(z).cpu().detach()\n",
    "show_images(output, first_title=\"Generated\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhRuUc1iXPi4"
   },
   "source": [
    "## 2.2. Latent Representation (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGcMLtWBHftV"
   },
   "source": [
    "Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\n",
    "Ваша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n",
    "\n",
    "Это позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n",
    "\n",
    "Плюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n",
    "\n",
    "Подсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n",
    "\n",
    "\n",
    "Итак, план:\n",
    "1. Получить латентные представления картинок тестового датасета\n",
    "2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n",
    "3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5FgK2JiHgV6"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJZ5n96LH0uS"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(test_set, batch_size=100, num_workers=2)\n",
    "vae.eval()\n",
    "latent_vectors_cnn, latent_vectors_fc = [], []\n",
    "with torch.no_grad():\n",
    "  for batch, _ in loader:\n",
    "    output_cnn = vae_cnn.get_latent_vector(batch.to(device).float())\n",
    "    output_fc = vae_fc.get_latent_vector(batch.to(device).float())\n",
    "    latent_vectors_cnn.append(output_cnn)\n",
    "    latent_vectors_fc.append(output_fc)\n",
    "latent_vectors_cnn = torch.cat(latent_vectors_cnn)\n",
    "latent_vectors_fc = torch.cat(latent_vectors_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYIrJM09LGMM"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFnUIbtCKt-_",
    "outputId": "d0958857-a1f8-4482-9414-ce1557fc602a"
   },
   "outputs": [],
   "source": [
    "view_cnn = tsne.fit_transform(latent_vectors_cnn.cpu().detach().numpy())\n",
    "view_fc = tsne.fit_transform(latent_vectors_fc.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h7sT_J8NQM5"
   },
   "outputs": [],
   "source": [
    "labels = torch.cat([y for _, y in loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "id": "r11MHsQ-Mr5u",
    "outputId": "fdca4c37-68fc-4eae-8708-3df5b0a4370b"
   },
   "outputs": [],
   "source": [
    "figure, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))\n",
    "sns.scatterplot(view_cnn[:, 0], view_cnn[:, 1], hue=labels, palette=sns.color_palette(), ax=ax1);\n",
    "sns.scatterplot(view_fc[:, 0], view_fc[:, 1], hue=labels, palette=sns.color_palette(), ax=ax2);\n",
    "ax1.set_title(\"VAE CNN latent vectors distrubution\")\n",
    "ax2.set_title(\"VAE FC latent vectors distrubution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85TVEE_FVlon"
   },
   "source": [
    "Что вы думаете о виде латентного представления?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p-Gg0TEV2ys"
   },
   "source": [
    "В целом результат довольно хороший. При использовании CNN VAE восстановление и генерация картинок проходит лучше, чем при использовании FC VAE, однако во втором случае латентные вектора каждого класса более четко разделяются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIYuKFwijN2U"
   },
   "source": [
    "## 2.3. Conditional VAE (6 баллов)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5l8Bu1RPjUx"
   },
   "source": [
    "Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \n",
    "Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \n",
    "И вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n",
    "\n",
    "Хотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n",
    "\n",
    "И в этой части задания мы научимся такие обучать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j8zNIwKPY-6"
   },
   "source": [
    "### Архитектура\n",
    "\n",
    "На картинке ниже представлена архитектура простого Conditional VAE.\n",
    "\n",
    "По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n",
    "\n",
    "То есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6YloFEAPeM4"
   },
   "source": [
    "\n",
    "![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n",
    "\n",
    "![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxg2tDSfRbLF"
   },
   "source": [
    "На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpFbSXLaPrm1"
   },
   "source": [
    "Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX0zxklMPwI2"
   },
   "source": [
    "P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKHhOHLM6gQ8"
   },
   "outputs": [],
   "source": [
    "class CVAE_CNN(nn.Module):\n",
    "\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=4), # 28 -> 25\n",
    "        nn.BatchNorm2d(num_features=8),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4), # 25 -> 22\n",
    "        nn.BatchNorm2d(num_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4), # 22 -> 19\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4), # 19 -> 16\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2), # 16 -> 8\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2), # 8 -> 4\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=32*4*4, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=2*self.latent_dim),\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=128, out_features=32*4*4),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(32, 4, 4)),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 8 -> 16\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=32), # 16 -> 14\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, # 14 -> 28\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        ConvBlock(in_channels=32, out_channels=1, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def encode(self, x, class_num):\n",
    "    output = self.encoder(torch.cat(x, class_num))\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    return mu, log_var, class_num\n",
    "\n",
    "  def gaussian_sampler(self, mu, log_var):\n",
    "    if self.training:\n",
    "      std = torch.exp(0.5 * log_var)\n",
    "      eps = torch.randn_like(mu)\n",
    "      return std * eps + mu\n",
    "    else:\n",
    "      return mu\n",
    "\n",
    "  def decode(self, z, class_num):\n",
    "    reconstruction = self.decoder(torch.cat(z, class_num))\n",
    "    return reconstruction\n",
    "\n",
    "  def forward(self, x, class_num):\n",
    "    mu, log_var, class_num = self.encode(x, class_num)\n",
    "    z = self.gaussian_sampler(mu. log_var)\n",
    "    reconstruction = self.decode(z, class_num)\n",
    "    return mu, log_var, reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBn0PxOY_3pW"
   },
   "outputs": [],
   "source": [
    "class CVAE_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(in_features=28*28 + 10, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=128, out_features=2*self.latent_dim)\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim + 10, out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=28*28),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n",
    "    )\n",
    "\n",
    "  def encode(self, x, class_num):\n",
    "    output = self.encoder(torch.cat((x.view(-1, 28*28), \n",
    "                                     F.one_hot(class_num, 10).view(-1, 10)), 1)) \\\n",
    "                                     .view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    return mu, log_var, class_num\n",
    "\n",
    "  def gaussian_sampler(self, mu, log_var):\n",
    "    if self.training:\n",
    "      std = torch.exp(0.5 * log_var)\n",
    "      eps = torch.randn_like(mu)\n",
    "      return std * eps + mu\n",
    "    else:\n",
    "      return mu\n",
    "\n",
    "  def decode(self, z, class_num):\n",
    "    reconstruction = self.decoder(torch.cat((z, F.one_hot(class_num, 10).view(-1, 10)), 1))\n",
    "    return reconstruction\n",
    "\n",
    "  def forward(self, x, class_num):\n",
    "    mu, log_var, class_num = self.encode(x, class_num)\n",
    "    z = self.gaussian_sampler(mu, log_var)\n",
    "    reconstruction = self.decode(z, class_num)\n",
    "    return mu, log_var, reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXBfCElwECz4"
   },
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su2EyOZnEczH"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, train_loader, summary_writer=None):\n",
    "  train_losses_epoch = []\n",
    "\n",
    "  model.train()\n",
    "  for i, (batch, labels) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    mu, log_var, reconstruction = model(batch.to(device).float(), labels.to(device))\n",
    "    loss = criterion(batch.to(device).float(), reconstruction, mu, log_var)\n",
    "    train_losses_epoch.append(loss.item())\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalar(\"Epoch. Train loss\", loss.item(), i)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  return train_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75oQvVvNEczI"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, criterion, optimizer, valid_loader, summary_writer=None):\n",
    "  valid_losses_epoch = []\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, (batch, labels) in enumerate(valid_loader):\n",
    "      mu, log_var, reconstruction = model(batch.to(device).float(), labels.to(device))\n",
    "      loss = criterion(batch.to(device).float(), reconstruction, mu, log_var)\n",
    "      valid_losses_epoch.append(loss.item())\n",
    "      if summary_writer is not None:\n",
    "        summary_writer.add_scalar(\"Epoch. Valid loss\", loss.item(), i)\n",
    "\n",
    "  return valid_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rcOvOzqEczJ"
   },
   "outputs": [],
   "source": [
    "def visualize(examples, labels, reconstructions, train_losses, valid_losses):\n",
    "  #plt.ion()\n",
    "  figure = plt.figure(constrained_layout=True, figsize=(32, 8))\n",
    "  subfigs = figure.subfigures(1, 2, wspace=0.07)\n",
    "  axs_left = subfigs[0].subplots(2, 5) \n",
    "  ax = subfigs[1].subplots(1, 1)\n",
    "  for j in range(5):\n",
    "      axs_left[0, j].clear()\n",
    "      axs_left[0, j].imshow(examples[j].view(28, 28).numpy())\n",
    "      axs_left[1, j].clear()\n",
    "      axs_left[1, j].imshow(reconstructions[j].view(28, 28).cpu().numpy())\n",
    "      for i in range(2):\n",
    "        axs_left[i, j].set_xticks([])\n",
    "        axs_left[i, j].set_yticks([])\n",
    "      axs_left[0, j].set_title(f\"Source. Label {labels[j]}\")\n",
    "      axs_left[1, j].set_title(\"Reconstruction\")\n",
    "\n",
    "  ax.clear()\n",
    "  ax.plot(train_losses, label=\"Train\")\n",
    "  ax.plot(valid_losses, label=\"Validation\")\n",
    "  ax.set_title(\"Training AE\", fontsize=18) \n",
    "  ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "  ax.set_ylabel(\"Loss value\", fontsize=14)\n",
    "  ax.legend()\n",
    "  #figure.canvas.draw()\n",
    "  #figure.canvas.flush_events()\n",
    "  plt.show()\n",
    "  return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d40_YftgEczM"
   },
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, train_loader, valid_loader, epochs, summary_writer=None):\n",
    "  train_losses, valid_losses = [], []\n",
    "\n",
    "  pbar = tqdm(range(epochs))\n",
    "  pbar.set_description(\"Epoch 1\")\n",
    "  for epoch in pbar:\n",
    "    if epoch != 0:\n",
    "      pbar.set_description(f\"Epoch {epoch + 1}. \\\n",
    "      Train loss: {round(train_losses[-1], 4)}. \\\n",
    "      Valid loss: {round(valid_losses[-1], 4)}\")\n",
    "\n",
    "    train_losses_epoch = train_epoch(model, criterion, optimizer, \n",
    "                                     train_loader, summary_writer)\n",
    "    valid_losses_epoch = valid_epoch(model, criterion, optimizer, \n",
    "                                     valid_loader, summary_writer)\n",
    "    \n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "    valid_losses.append(np.mean(valid_losses_epoch))\n",
    "\n",
    "    indices = np.random.randint(0, len(valid_loader.dataset), size=5)\n",
    "    examples = torch.stack([valid_loader.dataset[i][0] for i in indices])\n",
    "    labels = torch.tensor([valid_loader.dataset[i][1] for i in indices])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      _, _, reconstructions = model(examples.to(device).float(), labels.to(device))\n",
    "\n",
    "    figure = visualize(examples, labels, reconstructions, train_losses, valid_losses)\n",
    "\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalars(\"Training\", {\"Train\" : train_losses[-1],\n",
    "                                             \"Valid\" : valid_losses[-1]}, epoch)\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        summary_writer.add_figure(f\"Reconstruction. Epoch {epoch + 1}\", figure)\n",
    "    \n",
    "  return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "is-A_d80AlwA"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "cvae_fc = CVAE_FC().to(device)\n",
    "optimizer = optim.Adam(cvae_fc.parameters())\n",
    "criterion = loss_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnKsGh4EDqg7"
   },
   "outputs": [],
   "source": [
    "writer = tensorboard.SummaryWriter(\"./CVAE Experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6ac3aeb491f74009a42ea7f003cff150",
      "90347eac10f345cd84b483dd3c26c934",
      "371c9729c2974488bd6f38f11857fef0",
      "5c3e5b6de2c744c5bb5a090d4a691d5c",
      "9aef7cf7bb124d39b8d63436bf638453",
      "ec4e644ec365446ab40bf69c64679092",
      "735bd9c82ae240e3b2d55dc82bf58e6f",
      "e1a34840f6b34ebe8e8283852d2af80a",
      "11b77c44a8b24327a3ebe38e2c719bae",
      "b2a03f087e7b4601a4515f4290c3226e",
      "fdb09ac641ce4aef926a59f69c1c1a01"
     ]
    },
    "id": "VcsF45bEAuLu",
    "outputId": "1060eb36-35d7-46c6-b7d9-43f629a340bf"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses = fit(cvae_fc, criterion, optimizer, train_loader, test_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoMw-IFyP5A2"
   },
   "source": [
    "### Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe1zWyZHkLV2"
   },
   "source": [
    "Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\n",
    "Для MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynQ4d1GGWEhA"
   },
   "source": [
    "Давайте будем семплировать из одних векторов цифры всех классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0SQIhvNP9Dr",
    "outputId": "7ef3f97b-946d-4f0b-f066-c446ba3845f8"
   },
   "outputs": [],
   "source": [
    "z = torch.FloatTensor(np.array([np.random.randn(LATENT_DIM) for i in range(10)])).to(device)\n",
    "labels = torch.tensor(np.arange(0, 10))\n",
    "print(z.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "S96_QrOMahr4",
    "outputId": "79a502ad-acc8-46ca-9153-0598de210ef4"
   },
   "outputs": [],
   "source": [
    "figure, axs = plt.subplots(10, 10, figsize=(10, 10), \n",
    "                           gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    axs[i, j].imshow(cvae_fc.decode(z[i].view(1, -1).to(device), \n",
    "                                    labels[j].to(device))\\\n",
    "                     .view(28, 28).cpu().detach().numpy())\n",
    "    axs[i, j].set_xticks([])\n",
    "    axs[i, j].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAWBu8rzQBgQ"
   },
   "source": [
    "Splendid! Вы великолепны!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt2S77cm3O1v"
   },
   "source": [
    "### Latent Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt7x8Ek_rHTE"
   },
   "source": [
    "Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n",
    "\n",
    "Опять же, нужно покрасить точки в разные цвета в зависимости от класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXGNZZkrphFK"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(test_set, batch_size=100, num_workers=2)\n",
    "cvae_fc.eval()\n",
    "latent_vectors_fc = []\n",
    "with torch.no_grad():\n",
    "  for batch, labels in loader:\n",
    "    mu, log_var, _ = cvae_fc.encode(batch.to(device).float(), labels.to(device))\n",
    "    latent_vectors_fc.append(cvae_fc.gaussian_sampler(mu, log_var))\n",
    "latent_vectors_fc = torch.cat(latent_vectors_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQb_1NB_phFK"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KeulkvrephFK",
    "outputId": "b94875aa-1aa0-49ac-f4dc-9f78782236f1"
   },
   "outputs": [],
   "source": [
    "view_fc = tsne.fit_transform(latent_vectors_fc.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpg_RQgAphFL"
   },
   "outputs": [],
   "source": [
    "labels = torch.cat([y for _, y in loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "rCDfZvLJphFL",
    "outputId": "bcff06dc-ca32-418e-c98c-e92f47091961"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(view_fc[:, 0], view_fc[:, 1], hue=labels, palette=sns.color_palette());\n",
    "plt.title(\"CVAE FC latent vectors distrubution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET8IELWu3Z2c"
   },
   "source": [
    "Что вы думаете насчет этой картинки? Отличается от картинки для VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWkqHjvTCD_8"
   },
   "source": [
    "Отличие явно заметно. Теперь нет кластеров по классам. Здесь скорее уже идет распределение по стилистике изображения цифры, а их довольно много и они могу пересекаться, поэтому латентные вектора распределены так \"беспорядочно\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN3D_k5W_WZz"
   },
   "source": [
    "# BONUS 1: Denoising\n",
    "\n",
    "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a1jkpkCsIU"
   },
   "source": [
    "У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8EN-8jlCtmd"
   },
   "source": [
    "Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \n",
    "То есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1OJg6jhlaZl"
   },
   "source": [
    "<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysI0BCuRDbvm"
   },
   "source": [
    "Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n",
    "\n",
    "В питоне шум можно добавить так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5e746iVDgSm"
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcBmYLkN73M5"
   },
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBPAhppUu-B6"
   },
   "outputs": [],
   "source": [
    "def read_attributes(attrs_name = \"lfw_attributes.txt\",\n",
    "                  images_name = \"lfw-deepfunneled\"):\n",
    "    #Download if not exists\n",
    "    if not os.path.exists(images_name):\n",
    "        print(\"images not found, donwloading...\")\n",
    "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
    "        print(\"extracting...\")\n",
    "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
    "        print(\"done\")\n",
    "        assert os.path.exists(images_name)\n",
    "\n",
    "    if not os.path.exists(attrs_name):\n",
    "        print(\"attributes not found, downloading...\")\n",
    "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
    "        print(\"done\")\n",
    "\n",
    "    #Read attrs\n",
    "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "\n",
    "\n",
    "    #Read photos\n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    \n",
    "    #Merge (photos now have same order as attributes)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"Lost some data when merging dataframes\"\n",
    "    #all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJfP8Eh3u-B7"
   },
   "outputs": [],
   "source": [
    "attrs = read_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "fM2_FQP1u-B7",
    "outputId": "a2320c26-b808-4c53-fad8-578d6cf25d72"
   },
   "outputs": [],
   "source": [
    "attrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hCRk1iA4rJk"
   },
   "outputs": [],
   "source": [
    "class Noise():\n",
    "  def __init__(self, noise_factor=0.5, mean=0, std=1):\n",
    "    self.noise_factor = noise_factor\n",
    "    self.mean=mean\n",
    "    self.std = std\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    noisy_sample = sample + self.noise_factor * np.random.normal(self.mean, self.std, sample.shape)\n",
    "    return noisy_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_J0Q6Jfu-B7"
   },
   "outputs": [],
   "source": [
    "class FacesDataset(Dataset):\n",
    "  def __init__(self, filenames, size, noise=False):\n",
    "    self.filenames = filenames\n",
    "    self.size = size\n",
    "    self.noise = noise\n",
    "    transforms_array = [\n",
    "                        tfs.CenterCrop(110),\n",
    "                        tfs.Resize(size=self.size),\n",
    "                        tfs.ToTensor(),\n",
    "                        tfs.Normalize(mean=0, std=1)\n",
    "                        ]\n",
    "    if self.noise:\n",
    "      transforms_array.append(Noise(noise_factor=0.15))\n",
    "    self.transform = tfs.Compose(transforms_array)\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.filenames)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    \n",
    "    image_filename = self.filenames[idx]\n",
    "    image = Image.open(image_filename)\n",
    "    image.load()\n",
    "    image = self.transform(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiJpbUwlu-B7"
   },
   "source": [
    "Разделим непосредственно таблицу атрибутов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYwPtkDLu-B7"
   },
   "outputs": [],
   "source": [
    "train_attrs, valid_attrs = train_test_split(attrs, train_size=0.9, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBo0rDCpu-B7",
    "outputId": "6dfdc45e-0e76-4290-90d5-7722d32a3b80"
   },
   "outputs": [],
   "source": [
    "print(\"Train attributes shape: \", train_attrs.shape)\n",
    "print(\"Valid attributes shape: \", valid_attrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV2K89elu-B7"
   },
   "source": [
    "Создадим непосредственно датасеты для обучающей и валидацинной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-wu197su-B7"
   },
   "outputs": [],
   "source": [
    "train_set = FacesDataset(train_attrs[\"photo_path\"].values, size=128)\n",
    "valid_set = FacesDataset(valid_attrs[\"photo_path\"].values, size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2F7RFa_6w42"
   },
   "source": [
    "Также создадим зашумленные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEJg9QiJ62cp"
   },
   "outputs": [],
   "source": [
    "train_set_noisy = FacesDataset(train_attrs[\"photo_path\"].values, size=128, noise=True)\n",
    "valid_set_noisy = FacesDataset(valid_attrs[\"photo_path\"].values, size=128, noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8C7mAVt7u-B7"
   },
   "outputs": [],
   "source": [
    "def show_images(ground_truth, reconstructions=None, \n",
    "                first_title=\"Source\", second_title=\"Reconstruction\"):\n",
    "  if reconstructions is None:\n",
    "    size = 1\n",
    "  else: \n",
    "    size = 2\n",
    "  fig = plt.figure(figsize=(5 * ground_truth.shape[0], size * 5))\n",
    "  for i, image in enumerate(ground_truth):\n",
    "    #plt.title(\"Ground truth\")\n",
    "    plt.subplot(size, ground_truth.shape[0], i + 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(first_title)\n",
    "    plt.imshow(image.permute((1, 2, 0)).numpy())\n",
    "\n",
    "  if reconstructions is not None:\n",
    "    for i, image in enumerate(reconstructions):\n",
    "    #plt.title(\"Ground truth\")\n",
    "      plt.subplot(size, ground_truth.shape[0], ground_truth.shape[0] + i + 1)\n",
    "      plt.grid(False)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.title(second_title)\n",
    "      plt.imshow(image.permute((1, 2, 0)).cpu().detach().numpy())\n",
    "  plt.ioff()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCTN5-Pf7Gqp"
   },
   "source": [
    "Примеры оригинальных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTB6PwHhu-B7"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([train_set[i] for i in np.random.randint(0, len(train_set), size=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "V97o0zDeu-B8",
    "outputId": "161052a6-67ac-4e2a-b55e-3d6ccfb818fe"
   },
   "outputs": [],
   "source": [
    "show_images(examples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7N1TNLu7L2a"
   },
   "source": [
    "Примеры зашумленных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KczTFbdo7KXD"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([train_set_noisy[i] for i in np.random.randint(0, len(train_set), size=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "kMam99bX7KXD",
    "outputId": "d50108d7-55fb-448a-8d6a-a45449c6d8b0"
   },
   "outputs": [],
   "source": [
    "show_images(examples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKqA-MsBu-B8"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaeO4rTZ7srX"
   },
   "outputs": [],
   "source": [
    "train_loader_noisy = DataLoader(train_set_noisy, batch_size=128, shuffle=True)\n",
    "valid_loader_noisy = DataLoader(valid_set_noisy, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pdVNHTO773d"
   },
   "source": [
    "## Архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h64MWd-p8hy0"
   },
   "outputs": [],
   "source": [
    "#Latent space's dimension\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4--IIzc68hy1"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, padding=0):\n",
    "    super().__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.padding = padding\n",
    "    self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                          kernel_size=3, padding=self.padding)\n",
    "    self.bn = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.bn(x)\n",
    "    x = F.elu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51uH-roJA7n4"
   },
   "outputs": [],
   "source": [
    "class NoiseLambda(nn.Module):\n",
    "  def __init__(self, noise_factor=0.5, mean=0, std=1):\n",
    "    super().__init__()\n",
    "    self.noise_factor = noise_factor\n",
    "    self.mean=mean\n",
    "    self.std = std\n",
    "\n",
    "  def forward(self, sample):\n",
    "    noisy_sample = sample + self.noise_factor * \\\n",
    "    torch.tensor(np.random.normal(self.mean, self.std, sample.shape), device=device).float()\n",
    "    return noisy_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeEHLgN18hy1"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.noise_flag = True\n",
    "    self.noise = NoiseLambda()\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        ConvBlock(in_channels=3, out_channels=16),\n",
    "        ConvBlock(in_channels=16, out_channels=16),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=16, out_channels=32),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=32, out_channels=64),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=64, out_channels=64),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=64, out_channels=128),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=4*4*128, out_features=self.latent_dim),\n",
    "        nn.ELU()\n",
    "\n",
    "    )\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=4*4*128),\n",
    "        nn.ELU(),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(128, 4, 4)),\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        ConvBlock(in_channels=128, out_channels=64, padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=64, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, # 8 -> 16\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, # 16 -> 32\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=32, out_channels=16, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, # 32 -> 64\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=16, out_channels=8, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=3, # 64 -> 128\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        nn.Sigmoid()\n",
    "\n",
    "    )\n",
    "  \n",
    "  def change_noise(self, flag):\n",
    "    self.noise_flag = flag\n",
    "\n",
    "  def forward(self, sample):\n",
    "    if self.noise_flag:\n",
    "      sample = self.noise(sample)\n",
    "    latent = self.encoder(sample)\n",
    "    reconstructed = self.decoder(latent)\n",
    "    return reconstructed, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_RxcTeq8hy1",
    "outputId": "5a663783-a5e0-477b-c450-3d556e64962e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Training device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvFn9soXFcqn"
   },
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2rEUe6HFZSq",
    "outputId": "74d3aec2-5057-45c2-fc5e-4d4fac1f319e"
   },
   "outputs": [],
   "source": [
    "summary(autoencoder, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fvlIoMS957H"
   },
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nTZ7-XT-EDa"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, train_loader, summary_writer=None):\n",
    "  train_losses_epoch = []\n",
    "\n",
    "  model.train()\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    reconstruction, latent_code = model(batch.to(device).float())\n",
    "    loss = criterion(reconstruction, batch.to(device).float())\n",
    "    train_losses_epoch.append(loss.item())\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalar(\"Epoch. Train loss\", loss.item(), i)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  return train_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ne_HVYd-EDb"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, criterion, optimizer, valid_loader, summary_writer=None):\n",
    "  valid_losses_epoch = []\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "      reconstruction, latent_code = model(batch.to(device).float())\n",
    "      loss = criterion(reconstruction, batch.to(device).float())\n",
    "      valid_losses_epoch.append(loss.item())\n",
    "      if summary_writer is not None:\n",
    "        summary_writer.add_scalar(\"Epoch. Valid loss\", loss.item(), i)\n",
    "\n",
    "  return valid_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSGuWxeK-EDb"
   },
   "outputs": [],
   "source": [
    "def visualize(examples, reconstructions, train_losses, valid_losses):\n",
    "  #plt.ion()\n",
    "  figure = plt.figure(constrained_layout=True, figsize=(32, 8))\n",
    "  subfigs = figure.subfigures(1, 2, wspace=0.07)\n",
    "  axs_left = subfigs[0].subplots(2, 5) \n",
    "  ax = subfigs[1].subplots(1, 1)\n",
    "  for j in range(5):\n",
    "      axs_left[0, j].clear()\n",
    "      axs_left[0, j].imshow(examples[j].permute((1, 2, 0)).numpy())\n",
    "      axs_left[1, j].clear()\n",
    "      axs_left[1, j].imshow(reconstructions[j].permute((1, 2, 0)).cpu().numpy())\n",
    "      for i in range(2):\n",
    "        axs_left[i, j].set_xticks([])\n",
    "        axs_left[i, j].set_yticks([])\n",
    "      axs_left[0, j].set_title(\"Source\")\n",
    "      axs_left[1, j].set_title(\"Reconstruction\")\n",
    "\n",
    "  ax.clear()\n",
    "  ax.plot(train_losses, label=\"Train\")\n",
    "  ax.plot(valid_losses, label=\"Validation\")\n",
    "  ax.set_title(\"Training AE\", fontsize=18) \n",
    "  ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "  ax.set_ylabel(\"Loss value\", fontsize=14)\n",
    "  ax.legend()\n",
    "  figure.canvas.draw()\n",
    "  figure.canvas.flush_events()\n",
    "  plt.show()\n",
    "  return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GOBLlDj-EDb"
   },
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, train_loader, valid_loader, epochs, summary_writer=None):\n",
    "  train_losses, valid_losses = [], []\n",
    "\n",
    "  pbar = tqdm(range(epochs))\n",
    "  pbar.set_description(\"Epoch 1\")\n",
    "  for epoch in pbar:\n",
    "    if epoch != 0:\n",
    "      pbar.set_description(f\"Epoch {epoch + 1}. \\\n",
    "      Train loss: {round(train_losses[-1], 4)}. \\\n",
    "      Valid loss: {round(valid_losses[-1], 4)}\")\n",
    "\n",
    "    train_losses_epoch = train_epoch(model, criterion, optimizer, \n",
    "                                     train_loader, summary_writer)\n",
    "    valid_losses_epoch = valid_epoch(model, criterion, optimizer, \n",
    "                                     valid_loader, summary_writer)\n",
    "    \n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "    valid_losses.append(np.mean(valid_losses_epoch))\n",
    "\n",
    "    examples = torch.stack([valid_set_noisy[i] for i in np.random.randint(0, len(valid_set), size=5)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      reconstructions, latent_codes = model(examples.to(device).float())\n",
    "\n",
    "    figure = visualize(examples, reconstructions, train_losses, valid_losses)\n",
    "\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalars(\"Training\", {\"Train\" : train_losses[-1],\n",
    "                                             \"Valid\" : valid_losses[-1]}, epoch)\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        summary_writer.add_figure(f\"Reconstruction. Epoch {epoch + 1}\", figure)\n",
    "    \n",
    "  return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEb0YfZp-XuV"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(autoencoder.parameters())\n",
    "criterion = nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0g12HR9-EDb"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "writer = tensorboard.SummaryWriter(\"AE Denoising Experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b272543074604d109be9dc96c624b359",
      "e96788cb925f46c4abf5b3d468494424",
      "ee579a5a15a446218bf5fcd901bb4928",
      "f7ad830aa2154f2592e4535ae18ca46f",
      "8764c31dbdf24e879dd4b7aac979240d",
      "f813c614350d4ed98775919d60fa62e7",
      "fb6fd19093a246b99bb0a6b1539bbdc9",
      "bf2080e4949046869ebea637f08b558c",
      "57540beee75241f6b770925acf9367fa",
      "6ca3bb297df24cb88ad9ad16118a0a80",
      "8444b68504fd4e0493482ce005135003"
     ]
    },
    "id": "cSgmiLRJ-EDb",
    "outputId": "9ea16f2c-d781-46ba-8bd1-0e57402a12fe"
   },
   "outputs": [],
   "source": [
    "train_losses_mse, valid_losses_mse = fit(autoencoder, criterion, optimizer, train_loader, valid_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYa4QubGke1H"
   },
   "source": [
    "## Результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQEgR79nNNX4"
   },
   "outputs": [],
   "source": [
    "examples = torch.stack([valid_set_noisy[i] for i in np.random.randint(0, len(valid_set_noisy), size=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRfx_PSKktHq"
   },
   "outputs": [],
   "source": [
    "denoised, _ = autoencoder(examples.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "RAulVZaulQsJ",
    "outputId": "1e31e456-2800-44f5-d829-6b545dc3e0ee"
   },
   "outputs": [],
   "source": [
    "show_images(examples, denoised, first_title=\"Source\", second_title=\"Denoised\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehG9Bq7qu5xF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NDiCPYLm2bY"
   },
   "source": [
    "# BONUS 2: Image Retrieval\n",
    "\n",
    "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xao_27WMm7AL"
   },
   "source": [
    "Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y__bdS23ndeY"
   },
   "source": [
    "План:\n",
    "\n",
    "1. Получаем латентные представления всех лиц тренировочного датасета\n",
    "2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n",
    "3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n",
    "4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n",
    "5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I0CFaWPxWxf"
   },
   "outputs": [],
   "source": [
    "#Latent space's dimension\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K04clgBdxWxg"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, padding=0):\n",
    "    super().__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.padding = padding\n",
    "    self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                          kernel_size=3, padding=self.padding)\n",
    "    self.bn = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.bn(x)\n",
    "    x = F.elu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4F2Z5E_xWxg"
   },
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "  def __init__(self, latent_dim=LATENT_DIM):\n",
    "    super().__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        ConvBlock(in_channels=3, out_channels=16),\n",
    "        ConvBlock(in_channels=16, out_channels=16),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=16, out_channels=32),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=32, out_channels=64),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=64, out_channels=64),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ConvBlock(in_channels=64, out_channels=128),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=4*4*128, out_features=2*self.latent_dim),\n",
    "        nn.ELU()\n",
    "\n",
    "    )\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(in_features=self.latent_dim, out_features=4*4*128),\n",
    "        nn.ELU(),\n",
    "        nn.Unflatten(dim=1, unflattened_size=(128, 4, 4)),\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, \n",
    "                           stride=2, padding=1, output_padding=1), # 4 -> 8\n",
    "        ConvBlock(in_channels=128, out_channels=64, padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=64, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, # 8 -> 16\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=64, out_channels=32, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, # 16 -> 32\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=32, out_channels=16, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, # 32 -> 64\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        ConvBlock(in_channels=16, out_channels=8, padding=1),\n",
    "        nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=3, # 64 -> 128\n",
    "                           stride=2, padding=1, output_padding=1),\n",
    "        nn.Sigmoid()\n",
    "\n",
    "    )\n",
    "\n",
    "  def reparameterize(self, mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    sample = std * eps + mu\n",
    "    return sample\n",
    "\n",
    "  def get_latent_vector(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    return z\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "    mu = output[:, 0, :]\n",
    "    log_var = output[:, 1, :]\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction, mu, log_var\n",
    "\n",
    "  def sample(self, z):\n",
    "    generated = self.decoder(z)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyhXLuGuJEGT"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, train_loader, summary_writer=None):\n",
    "  train_losses_epoch = []\n",
    "\n",
    "  model.train()\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    reconstruction, mu, log_var = model(batch.to(device).float())\n",
    "    loss = criterion(batch.to(device).float(), reconstruction, mu, log_var)\n",
    "    train_losses_epoch.append(loss.item())\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalar(\"Epoch. Train loss\", loss.item(), i)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  return train_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGtI-uQPJEGU"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, criterion, optimizer, valid_loader, summary_writer=None):\n",
    "  valid_losses_epoch = []\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "      reconstruction, mu, log_var = model(batch.to(device).float())\n",
    "      loss = criterion(batch.to(device).float(), reconstruction, mu, log_var)\n",
    "      valid_losses_epoch.append(loss.item())\n",
    "      if summary_writer is not None:\n",
    "        summary_writer.add_scalar(\"Epoch. Valid loss\", loss.item(), i)\n",
    "\n",
    "  return valid_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FByXb2tJEGU"
   },
   "outputs": [],
   "source": [
    "def visualize(examples, reconstructions, train_losses, valid_losses):\n",
    "  #plt.ion()\n",
    "  figure = plt.figure(constrained_layout=True, figsize=(32, 8))\n",
    "  subfigs = figure.subfigures(1, 2, wspace=0.07)\n",
    "  axs_left = subfigs[0].subplots(2, 5) \n",
    "  ax = subfigs[1].subplots(1, 1)\n",
    "  for j in range(5):\n",
    "      axs_left[0, j].clear()\n",
    "      axs_left[0, j].imshow(examples[j].permute(1, 2, 0).numpy())\n",
    "      axs_left[1, j].clear()\n",
    "      axs_left[1, j].imshow(reconstructions[j].permute(1, 2, 0).cpu().numpy())\n",
    "      for i in range(2):\n",
    "        axs_left[i, j].set_xticks([])\n",
    "        axs_left[i, j].set_yticks([])\n",
    "      axs_left[0, j].set_title(\"Source\")\n",
    "      axs_left[1, j].set_title(\"Reconstruction\")\n",
    "\n",
    "  ax.clear()\n",
    "  ax.plot(train_losses, label=\"Train\")\n",
    "  ax.plot(valid_losses, label=\"Validation\")\n",
    "  ax.set_title(\"Training AE\", fontsize=18) \n",
    "  ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "  ax.set_ylabel(\"Loss value\", fontsize=14)\n",
    "  ax.legend()\n",
    "  #figure.canvas.draw()\n",
    "  #figure.canvas.flush_events()\n",
    "  plt.show()\n",
    "  return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqsZBQHkJEGU"
   },
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, train_loader, valid_loader, epochs, summary_writer=None):\n",
    "  train_losses, valid_losses = [], []\n",
    "\n",
    "  pbar = tqdm(range(epochs))\n",
    "  pbar.set_description(\"Epoch 1\")\n",
    "  for epoch in pbar:\n",
    "    if epoch != 0:\n",
    "      pbar.set_description(f\"Epoch {epoch + 1}. \\\n",
    "      Train loss: {round(train_losses[-1], 4)}. \\\n",
    "      Valid loss: {round(valid_losses[-1], 4)}\")\n",
    "\n",
    "    train_losses_epoch = train_epoch(model, criterion, optimizer, \n",
    "                                     train_loader, summary_writer)\n",
    "    valid_losses_epoch = valid_epoch(model, criterion, optimizer, \n",
    "                                     valid_loader, summary_writer)\n",
    "    \n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "    valid_losses.append(np.mean(valid_losses_epoch))\n",
    "\n",
    "    examples = torch.stack([valid_loader.dataset[i]\n",
    "                            for i in np.random.randint(0, len(valid_loader.dataset), size=5)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      reconstructions, _, _ = model(examples.to(device).float())\n",
    "\n",
    "    figure = visualize(examples, reconstructions, train_losses, valid_losses)\n",
    "\n",
    "    if summary_writer is not None:\n",
    "      summary_writer.add_scalars(\"Training\", {\"Train\" : train_losses[-1],\n",
    "                                             \"Valid\" : valid_losses[-1]}, epoch)\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        summary_writer.add_figure(f\"Reconstruction. Epoch {epoch + 1}\", figure)\n",
    "    \n",
    "  return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IA7bEMIIyGnJ"
   },
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "criterion = loss_vae\n",
    "optimizer = optim.Adam(params=vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FB43FSfoPNFi"
   },
   "outputs": [],
   "source": [
    "vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ibvhd4nNLl43",
    "outputId": "4a4a3c05-efed-44cd-855f-417cd37710e6"
   },
   "outputs": [],
   "source": [
    "summary(vae, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqRFRzA8yGnJ"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "writer = tensorboard.SummaryWriter(log_dir=\"VAE Retrieval Experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "072baf336fd646eda9024bb6d8cd0c95",
      "03f672a2038745f5aa4e2043ff75e365",
      "59a4f268f3624622a94fc6fe046ae2eb",
      "67c035ada5b1453ab1d98e37b83e2993",
      "dfefbee770374d29a19edd6f58a1a3a5",
      "71b5619419594d51a459130de84c8b96",
      "3f95717a4c9a4300b1cb1642c8aeb969",
      "6aa9b8dadd0d4c39a5931bf95dc65861",
      "9929603fc7e444c196e21983549334f3",
      "2120eaf59c0d4e9fab361ebabf563dc9",
      "2cd26a5bab534fb8abe2fd5d3e9ae7c0"
     ]
    },
    "id": "r-dw228eyGnJ",
    "outputId": "17ab1640-90e3-44f1-c0e6-df1005f3f4be"
   },
   "outputs": [],
   "source": [
    "train_losses, valid_losses = fit(vae, criterion, optimizer, \n",
    "                                 train_loader, valid_loader, EPOCHS, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox7nxk8G3FyN"
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"model_state.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNuHybRF5uSX",
    "outputId": "91c35862-bbe3-4eae-faca-d480689649c4"
   },
   "outputs": [],
   "source": [
    "vae.load_state_dict(torch.load(\"model_state.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IksC2ucIoND-"
   },
   "source": [
    "Немного кода вам в помощь: (feel free to delete everything and write your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlJOSRUB9hqv"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBRQpK7ZPdzO",
    "outputId": "a69ef87d-3b6a-4bd8-e635-d624be036eef"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK0YpLMRoEa0"
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "  codes = [vae.get_latent_vector(batch.to(device).float()) for batch in train_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0JYsEBfSNfG"
   },
   "outputs": [],
   "source": [
    "codes = torch.cat(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uBHGPLz6zMH",
    "outputId": "5a2dea1c-35a9-4834-a58d-3a94a2297e4c"
   },
   "outputs": [],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KisDrgZdoWdt"
   },
   "outputs": [],
   "source": [
    "#обучаем LSHForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors().fit(codes.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_S5zPb5obam"
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n",
    "  # прогоняет векторы через декодер и получает картинки ближайших людей\n",
    "\n",
    "  code = vae.get_latent_vector(image.view(-1, 3, 128, 128).to(device).float())\n",
    "\n",
    "  (distances,), (idx,) = nn.kneighbors(code.cpu().detach().numpy(), n_neighbors=n_neighbors)\n",
    "\n",
    "  return distances, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2kjV5wupLP_"
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "\n",
    "  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n",
    "    \n",
    "    distances, neighbors_indices = get_similar(image, n_neighbors=11)\n",
    "    \n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(train_set[neighbors_indices[i]].cpu().numpy().transpose([1,2,0]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "X3kAsCzPBYir",
    "outputId": "2e9efbad-f1a2-4ca6-af52-c80127f9a9e5"
   },
   "outputs": [],
   "source": [
    "#print(np.random.randint(0, len(valid_set), 1).item())\n",
    "example = valid_set[np.random.randint(0, len(valid_set), 1).item()]\n",
    "show_similar(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJVcIOvfvrvM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03f672a2038745f5aa4e2043ff75e365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71b5619419594d51a459130de84c8b96",
      "placeholder": "​",
      "style": "IPY_MODEL_3f95717a4c9a4300b1cb1642c8aeb969",
      "value": "Epoch 100.       Train loss: 3702474.9153.       Valid loss: 3486654.3523: 100%"
     }
    },
    "04d1ee6b1bf44a3dac6cf7ab93e65275": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d7bc3751a94498596631e05072b8d8e",
      "placeholder": "​",
      "style": "IPY_MODEL_0ef8572a2b6f4bc4bad2cd2c9ae3be0e",
      "value": "Epoch 100.       Train loss: 0.0018.       Valid loss: 0.0027: 100%"
     }
    },
    "06ba3ff55e3e4979abb92b91602b8db8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "072baf336fd646eda9024bb6d8cd0c95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03f672a2038745f5aa4e2043ff75e365",
       "IPY_MODEL_59a4f268f3624622a94fc6fe046ae2eb",
       "IPY_MODEL_67c035ada5b1453ab1d98e37b83e2993"
      ],
      "layout": "IPY_MODEL_dfefbee770374d29a19edd6f58a1a3a5"
     }
    },
    "0cd6bcaa7a754b70bf05e0598db14eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_133c1353bdc04d8fb414a9e5ce555e0c",
       "IPY_MODEL_7fc0452c80d747d6a6973abc9c52b3de",
       "IPY_MODEL_d3e58903291b4ed98e3764c462db452b"
      ],
      "layout": "IPY_MODEL_e3e63a5324214cd8bbce423c4caf7e5d"
     }
    },
    "0ef8572a2b6f4bc4bad2cd2c9ae3be0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10821e7d9ac34f59b40b724e7d76866f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11b77c44a8b24327a3ebe38e2c719bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "133c1353bdc04d8fb414a9e5ce555e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_538fe3f2af4243e69c4b503f63eb932c",
      "placeholder": "​",
      "style": "IPY_MODEL_10821e7d9ac34f59b40b724e7d76866f",
      "value": "Epoch 100.       Train loss: 0.0041.       Valid loss: 0.0054: 100%"
     }
    },
    "1b03a157d15041578f6c7ee059d51470": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2120eaf59c0d4e9fab361ebabf563dc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27ea256f6a92472d95901c21cb4001bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b38392c574c4d31bce53dfcd5230c02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cd26a5bab534fb8abe2fd5d3e9ae7c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33ae86bfe41a4ad89ad8592b2114c9b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3473495fc1f046719fdec3a8939b8efa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "371c9729c2974488bd6f38f11857fef0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1a34840f6b34ebe8e8283852d2af80a",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_11b77c44a8b24327a3ebe38e2c719bae",
      "value": 100
     }
    },
    "3f95717a4c9a4300b1cb1642c8aeb969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4767fdd51e70431fb028e526d1e11931": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8d02a2a31cc45a988b64e34b4596f4c",
      "placeholder": "​",
      "style": "IPY_MODEL_628ae6705d4b4bf9b3d711b6b00a27b8",
      "value": "Epoch 100.       Train loss: 107654.5591.       Valid loss: 108310.7992: 100%"
     }
    },
    "4aab3f8f97114dc299aabd7b2fab5013": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "510e15337cc04519a1079db58f24f32d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "538fe3f2af4243e69c4b503f63eb932c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56e45dc638e34e639c0c1601be26eb3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57540beee75241f6b770925acf9367fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "589bcf5b03fe4e1fb8ecc385839c75ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59a4f268f3624622a94fc6fe046ae2eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6aa9b8dadd0d4c39a5931bf95dc65861",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9929603fc7e444c196e21983549334f3",
      "value": 100
     }
    },
    "5c3e5b6de2c744c5bb5a090d4a691d5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2a03f087e7b4601a4515f4290c3226e",
      "placeholder": "​",
      "style": "IPY_MODEL_fdb09ac641ce4aef926a59f69c1c1a01",
      "value": " 100/100 [07:20&lt;00:00,  4.56s/it]"
     }
    },
    "5d7bc3751a94498596631e05072b8d8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "628ae6705d4b4bf9b3d711b6b00a27b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67c035ada5b1453ab1d98e37b83e2993": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2120eaf59c0d4e9fab361ebabf563dc9",
      "placeholder": "​",
      "style": "IPY_MODEL_2cd26a5bab534fb8abe2fd5d3e9ae7c0",
      "value": " 100/100 [1:14:02&lt;00:00, 45.61s/it]"
     }
    },
    "6aa9b8dadd0d4c39a5931bf95dc65861": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac3aeb491f74009a42ea7f003cff150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_90347eac10f345cd84b483dd3c26c934",
       "IPY_MODEL_371c9729c2974488bd6f38f11857fef0",
       "IPY_MODEL_5c3e5b6de2c744c5bb5a090d4a691d5c"
      ],
      "layout": "IPY_MODEL_9aef7cf7bb124d39b8d63436bf638453"
     }
    },
    "6ca3bb297df24cb88ad9ad16118a0a80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71b5619419594d51a459130de84c8b96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "735bd9c82ae240e3b2d55dc82bf58e6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73c26c0a0b344636b69739aff5dbe9c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea1b939539d44948886fa4c2c2028d85",
      "placeholder": "​",
      "style": "IPY_MODEL_910f162b015547278e3da9f70b13e1e2",
      "value": " 100/100 [25:26&lt;00:00, 17.31s/it]"
     }
    },
    "7b037ed839064c779d35fa213d74cb2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fc0452c80d747d6a6973abc9c52b3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b38392c574c4d31bce53dfcd5230c02",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_510e15337cc04519a1079db58f24f32d",
      "value": 100
     }
    },
    "7ff34cdba50d48b1aa9c9ae37e2e1877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "834402d19f234cf4bbfe56287199cce8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8444b68504fd4e0493482ce005135003": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8764c31dbdf24e879dd4b7aac979240d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89e4b6e357014d75bdaa6c62eec16720": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90347eac10f345cd84b483dd3c26c934": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec4e644ec365446ab40bf69c64679092",
      "placeholder": "​",
      "style": "IPY_MODEL_735bd9c82ae240e3b2d55dc82bf58e6f",
      "value": "Epoch 100.       Train loss: 101859.8048.       Valid loss: 98340.9289: 100%"
     }
    },
    "910f162b015547278e3da9f70b13e1e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "975851b980b24a1089d868910cac516a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9deff164f6f436090726ac266284eb0",
       "IPY_MODEL_ee6bc4f982ef47bf8858fee38cf8119d",
       "IPY_MODEL_73c26c0a0b344636b69739aff5dbe9c3"
      ],
      "layout": "IPY_MODEL_06ba3ff55e3e4979abb92b91602b8db8"
     }
    },
    "97c20e9edc854213a755b1529ad9866d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9929603fc7e444c196e21983549334f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9aef7cf7bb124d39b8d63436bf638453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a508d397f20f4e399eb168c139455e74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_834402d19f234cf4bbfe56287199cce8",
      "placeholder": "​",
      "style": "IPY_MODEL_56e45dc638e34e639c0c1601be26eb3d",
      "value": " 100/100 [50:40&lt;00:00, 32.25s/it]"
     }
    },
    "a5d1ff60f8294372aa8d5371d0cc32de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4767fdd51e70431fb028e526d1e11931",
       "IPY_MODEL_c3394a2861e3495a8906f7ea7c82ccd3",
       "IPY_MODEL_c0033c4c7410499d8ee0d0fcfdd1a2ec"
      ],
      "layout": "IPY_MODEL_97c20e9edc854213a755b1529ad9866d"
     }
    },
    "a8d02a2a31cc45a988b64e34b4596f4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9deff164f6f436090726ac266284eb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3473495fc1f046719fdec3a8939b8efa",
      "placeholder": "​",
      "style": "IPY_MODEL_c3ce46f6cc0f4167891879f8c927a431",
      "value": "Epoch 100.       Train loss: 526920.8406.       Valid loss: 526810.9094: 100%"
     }
    },
    "b272543074604d109be9dc96c624b359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e96788cb925f46c4abf5b3d468494424",
       "IPY_MODEL_ee579a5a15a446218bf5fcd901bb4928",
       "IPY_MODEL_f7ad830aa2154f2592e4535ae18ca46f"
      ],
      "layout": "IPY_MODEL_8764c31dbdf24e879dd4b7aac979240d"
     }
    },
    "b2a03f087e7b4601a4515f4290c3226e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7b0b581ceac4572a8f35dbeac9d8533": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc80c74e3e05415a9db7bdee33af78f0",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27ea256f6a92472d95901c21cb4001bb",
      "value": 100
     }
    },
    "b9241467505f4e62988e34394bcf1ee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04d1ee6b1bf44a3dac6cf7ab93e65275",
       "IPY_MODEL_b7b0b581ceac4572a8f35dbeac9d8533",
       "IPY_MODEL_a508d397f20f4e399eb168c139455e74"
      ],
      "layout": "IPY_MODEL_589bcf5b03fe4e1fb8ecc385839c75ad"
     }
    },
    "bf2080e4949046869ebea637f08b558c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0033c4c7410499d8ee0d0fcfdd1a2ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5f74fdcfc7b43ac8afb0f520bf9502e",
      "placeholder": "​",
      "style": "IPY_MODEL_4aab3f8f97114dc299aabd7b2fab5013",
      "value": " 100/100 [07:41&lt;00:00,  4.53s/it]"
     }
    },
    "c3394a2861e3495a8906f7ea7c82ccd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b03a157d15041578f6c7ee059d51470",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ff34cdba50d48b1aa9c9ae37e2e1877",
      "value": 100
     }
    },
    "c3ce46f6cc0f4167891879f8c927a431": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d38c3bb958cb4b31b39f775cf00a9bc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3e58903291b4ed98e3764c462db452b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33ae86bfe41a4ad89ad8592b2114c9b5",
      "placeholder": "​",
      "style": "IPY_MODEL_d38c3bb958cb4b31b39f775cf00a9bc0",
      "value": " 100/100 [1:14:21&lt;00:00, 43.72s/it]"
     }
    },
    "dc80c74e3e05415a9db7bdee33af78f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfefbee770374d29a19edd6f58a1a3a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1a34840f6b34ebe8e8283852d2af80a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3e63a5324214cd8bbce423c4caf7e5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e96788cb925f46c4abf5b3d468494424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f813c614350d4ed98775919d60fa62e7",
      "placeholder": "​",
      "style": "IPY_MODEL_fb6fd19093a246b99bb0a6b1539bbdc9",
      "value": "Epoch 100.       Train loss: 0.0029.       Valid loss: 0.0036: 100%"
     }
    },
    "ea1b939539d44948886fa4c2c2028d85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec4e644ec365446ab40bf69c64679092": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee579a5a15a446218bf5fcd901bb4928": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf2080e4949046869ebea637f08b558c",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57540beee75241f6b770925acf9367fa",
      "value": 100
     }
    },
    "ee6bc4f982ef47bf8858fee38cf8119d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89e4b6e357014d75bdaa6c62eec16720",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b037ed839064c779d35fa213d74cb2a",
      "value": 100
     }
    },
    "f5f74fdcfc7b43ac8afb0f520bf9502e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7ad830aa2154f2592e4535ae18ca46f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ca3bb297df24cb88ad9ad16118a0a80",
      "placeholder": "​",
      "style": "IPY_MODEL_8444b68504fd4e0493482ce005135003",
      "value": " 100/100 [1:45:55&lt;00:00, 62.17s/it]"
     }
    },
    "f813c614350d4ed98775919d60fa62e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb6fd19093a246b99bb0a6b1539bbdc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdb09ac641ce4aef926a59f69c1c1a01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
